
\section{Theory}
\label{sec:sdf_estimation_cash_flows}

In this subsection, we shed light on the peculiarities of SDF discounting when pricing cash flow streams instead of returns.
Primarily, we want to consider multi-period and multi-cash-flows settings rather than a simple one-period return model.
In the new multi-period setting, we have to solve nonlinear optimizations when estimating the SDF parameters, which are much harder than the linear problems for one-period returns\footnote{Similarly, \cite{GSW19} identify multi-period discounting of cash flows as the reason for their "NPV-based inference bias."}.
Moreover, we analyze under which conditions we can replace a traditional net present value calculation with a net future value ($\Psi_{\tau,t}$ with $\tau>0$).
In contrast, the typical SDF formulas from the previous sections of this chapter always discount all cash flows to the starting date ($\Psi_{\tau,t}$ with $\tau=0$).
Our findings will be especially relevant for our SDF estimator in Chapter \ref{chap:spatial_sdf_estimator}.

The basic problem boils down to the fact that $\mathbb{E} \left[ \Psi \cdot CF \right] \neq \mathbb{E} \left[ \Psi \right] \mathbb{E} \left[ CF \right]$ due to the covariance term already known from Equation \ref{eq:sdf_covariance}.
Moreover, our problem is related to the so-called "Weitzman-Gollier puzzle," which discusses if an Expected Net Present Value or an Expected Net Future Value rule shall be applied to value long-term (social or environmental) investments, e.g., to mitigate climate change \citep{PR75,GW10,F10}. 
However, \cite{S20b} finds that the Weitzman-Gollier Puzzle "is not a puzzle, but the predictable consequence of ignoring the fact that the expectation of the inverses is not equal to the inverse of the expectation", which is known from Jensen's inequality $\mathbb{E} \left[ \frac{1}{\Psi} \right] \geq \frac{1}{\mathbb{E} \left[ \Psi \right]}$.

Another stream of literature, which is loosely connected to our problem, considers SDF pricing of financial assets over (very) long time horizons \citep{V02,CV04,HHL08,HS09,P09,M12}.
Moreover, \cite{G21} and \cite{GL21} use a cash flow duration factor to explain other well-known factors, like value, profitability, investment, low-risk, and payout factors. 
These papers indicate that the correct pricing of cash flows also has important implications in the public equity market.
From this viewpoint, it is more informative to model a given asset rather as future cash flow stream than as value process.
This branch of literature could be called "stochastic discount factor methods for traded cash flows."


\subsection{Two-period example with general SDF}


Let us analyze a simple two-period example with only two cash flows $CF=\left( {CF}_{t_1}, {CF}_{t_2} \right)$ in periods $t_1$ and $t_2$.
We have $T=t_2$.
The reference period for every SDF $\Psi$ is $t_0$.
Thus, $\Psi_{t_0}=1$.
All expectations are taken with $t_0$-information $\mathbb{E} \left[ . \right] \equiv \mathbb{E} \left[ . | \mathcal{F}_{t_0}  \right]$, if not stated otherwise.
\begin{assume}
	\label{ass:two_period_example}
	We make the following three assumptions: \\
	\begin{enumerate}
		\item all cash flows are generated by an adapted and predictable trading strategy $\delta$ as outlined in Definition \ref{def:trading_strategy}, i,e., $CF_{t_1}=-S_{t_1}^{\delta}$ and $CF_{t_2}=S_{t_2}^{\delta}$,
		\item the multi-period Riesz representation Theorem \ref{theo:riesz_multi_period} holds, i.e., 
		\[
		\pi(CF) = \mathbb{E} \left[ \Psi_{t_1} {CF}_{t_1} + \Psi_{t_2} {CF}_{t_2} \right]
		\]
		\item the martingale property from Theorem \ref{theo:martingal_property} holds, i.e., $(\Psi_t S_{t}^{\delta})_{0 \leq t \leq T}$ is a martingale (with respect to the original probability measure) for all $i=1,2,\dots,N$.
	\end{enumerate}
\end{assume}

\begin{lemma}
	The fair price $\pi(CF)$ of our cash flow $CF$ is zero, or equivalently
	$\mathbb{E} \left[ \Psi_{t_1} CF_{t_1} \right] = - \mathbb{E} \left[ \Psi_{t_2} CF_{t_2} \right]$.
\end{lemma}

\begin{proof}
	The lemma directly follows from the data generating process specified in Assumption \ref{ass:two_period_example} in combination with Theorem \ref{theo:nac}, which states that all trading strategies without initial capital have a zero price. 
	$\square$
\end{proof}

Next, we define the time-$\tau$ pricing error for our two cash flows as
\[
\epsilon_{\tau} = \frac{\Psi_{t_1} {CF}_{t_1} + \Psi_{t_2} {CF}_{t_2}}{\Psi_{\tau}}
\]
\textcolor{darkgreen}{where the discounted cash flows in the numerator get "compounded" to date $\tau$ by the term $\Psi_{\tau}$ in the denominator.}
Now we have several choices for $\tau \in \{t_0,t_1,t_2\}$.
The Net Present Value (NPV) case corresponds to $\mathcal{T}^{(A)}=\{ t_1 \}$ which yields
\[
\bar{\epsilon}^{(A)}
=
\epsilon_{\tau = t_1}
= 
\frac{\Psi_{t_1} {CF}_{t_1} + \Psi_{t_2} {CF}_{t_2}}{\Psi_{t_1}}
=
{CF}_{t_1} + \frac{\Psi_{t_2}}{\Psi_{t_1}}  {CF}_{t_2}
\]
The Net Future Value (NFV) case corresponds to $\mathcal{T}^{(B)}=\{ t_2 \}$ which yields
\[
\bar{\epsilon}^{(B)}
=
\epsilon_{\tau = t_2}
=
\frac{\Psi_{t_1} {CF}_{t_1} + \Psi_{t_2} {CF}_{t_2}}{\Psi_{t_2}}
=
\frac{\Psi_{t_1}}{\Psi_{t_2}} {CF}_{t_1} + {CF}_{t_2}
\]

\begin{proposition}
	The $t_0$-price of the pricing error $\epsilon$ has to be zero for all times $\tau$, i.e., 
	$\mathbb{E} \left[ \Psi_{\tau} \epsilon_{\tau} \right] = \mathbb{E} \left[ \Psi_{\tau} \right] \mathbb{E} \left[ \epsilon_{\tau} \right] + \mathrm{Cov} \left[ \Psi_{\tau}, \epsilon_{\tau} \right] = 0 \quad \forall \quad \tau \in \{ t_0 ,t_1 ,t_2 \}$.
\end{proposition}

\begin{proof}
	For the case $\tau = t_0$, $\mathbb{E} \left[ \Psi_{\tau} \epsilon_{\tau = t_0} \right]=0$ trivially holds since $\Psi_{t_0}=1$ by definition.
	Let us interpret $\epsilon_{\tau = t_2}$ as cash flow that occurs at $t_2$.
	The fair price of this cash flow is (at time $t_0$)
	\[
	\pi(\epsilon_{\tau = t_2}) = 
	\mathbb{E} \left[ \Psi_{t_2} \epsilon_{\tau = t_2} \right] = 
	\mathbb{E} \left[ \Psi_{t_2} \left( \frac{\Psi_{t_1}}{\Psi_{t_2}} {CF}_{t_1} + {CF}_{t_2} \right) \right]
	= 0
	\]
	Similarly, for $\epsilon_{\tau = t_1}$ we receive also $\pi(\epsilon_{\tau = t_1}) = \mathbb{E} \left[ \Psi_{t_1} \epsilon_{\tau = t_1} \right] =  0$. 
	$\square$
\end{proof}

\begin{corollary}
	Just for $\tau=t_0$, we \textcolor{darkgreen}{always obtain} $\mathbb{E} \left[ \epsilon_{\tau} \right]=0$.
\end{corollary}

\begin{proof}
	For $\tau=t_0$, we know $\mathbb{E} \left[ \epsilon_{\tau} \right]=0$ since the time-$t_0$ SDF is a constant $\Psi_{t_0}=1$.
	Generally, $\mathbb{E} \left[ \epsilon_{\tau} \right] =  - \frac{\mathrm{Cov} \left[ \Psi_{\tau}, \epsilon_{\tau} \right]}{\mathbb{E} \left[ \Psi_{\tau} \right]}$ which \textcolor{darkgreen}{usually} is nonzero for $\tau > t_0$.
	$\square$
\end{proof}

Thus, we generally get
\[
\mathbb{E} \left[ \sum_{\tau=t_0}^T  \epsilon_{\tau} \right] = 
\sum_{\tau=t_0}^T \mathbb{E} \left[ \epsilon_{\tau} \right] = 
\sum_{\tau=t_0}^T - \frac{\mathrm{Cov} \left[ \Psi_{\tau}, \epsilon_{\tau} \right]}{\mathbb{E} \left[ \Psi_{\tau} \right]}
\]
Just for the two-period special case, we can prove the following lemma.

\begin{lemma}
	It holds $\mathbb{E} \left[ \epsilon_{\tau = t_1} | \mathcal{F}_{t_1} \right] = 0$.
\end{lemma}

\begin{proof}
	Since we use $\mathcal{F}_{t_1}$ information, $CF_{t_1}$ and $\Psi_{t_1}$ are deterministic (realized random variables).
	From the martingale property, we know $\Psi_{t_1} S_{t_1}^{\delta} = \mathbb{E} \left[ \Psi_{t_2} S_{t_2}^{\delta} | \mathcal{F}_{t_1} \right]$.
	By resubstituting the trading strategies to cash flows and dividing both sides by $\Psi_{t_1}$, we receive $CF_{t_1} = - \mathbb{E} \left[ \frac{\Psi_{t_2}}{\Psi_{t_1}}  {CF}_{t_2} | \mathcal{F}_{t_1} \right]$ and consequentially $\mathbb{E} \left[ \epsilon_{\tau = t_1} | \mathcal{F}_{t_1} \right] = 0$. $\square$
\end{proof}

Again we face the general problem that we cannot observe the true deal/fund valuations between the cash flow transaction dates.
Just in our simple two-period example, the cash flow return is given by
\[
R = \frac{S_{t_2}^{\delta}}{S_{t_1}^{\delta}} = - \frac{CF_{t_2}}{CF_{t_1}} =
\frac{\Psi_{t_1}}{\Psi_{t_2}} - \frac{\Psi_{\tau} \epsilon_{\tau}}{\Psi_{t_2} CF_{t_1}}
\]
which yields the following decomposition for the expected return
\[
\mathbb{E} \left[ R \right] = 
\mathbb{E} \left[ - \frac{CF_{t_2}}{CF_{t_1}} \right] =
\mathbb{E} \left[ \frac{\Psi_{t_1}}{\Psi_{t_2}} \right] -
\left(
\mathbb{E} \left[ \Psi_{\tau} \epsilon_{\tau} \right]
\mathbb{E} \left[ \frac{1}{\Psi_{t_2}CF_{t_1}} \right]
+
\mathrm{Cov} \left[ \Psi_{\tau} \epsilon_{\tau},\frac{1}{\Psi_{t_2}CF_{t_1}} \right]
\right)
\]
This example shows that it is not straightforward to translate a general SDF to return expectations, even in our simple case.
Thus we conclude with the following central insight.

\begin{remark}
	\textcolor{darkgreen}{To simplify our estimation problem in the future, we can think of our SDF estimator as an (expected) return estimator.}
	This corresponds to $\Psi \equiv \frac{1}{R}$ and the following pricing relation
	\[
	\mathbb{E} \left[ \Psi R \right] = \mathbb{E} \left[ \frac{1}{R} R \right] = 
	\mathbb{E} \left[ \frac{1}{R} \right] \mathbb{E} \left[ R \right] + \mathrm{Cov} \left[ \frac{1}{R}, R \right] = 1
	\]
	where we cannot ignore the covariance term since always $\mathbb{E} \left[ \frac{1}{R} \right] \mathbb{E} \left[ R \right] \geq 1$.
\end{remark}

We pursue the $\mathbb{E} \left[ \frac{1}{R} R \right]$ pricing idea in the next subsection(s).


\subsection{Multiple deal-level cash flows with trivial SDF}
\label{subsec:sdf_estimation_multi_cfs_trivial_sdf}


Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a discrete-time probability space, $T \in \mathbb{N}>0$.
The filtration $\mathbb{F}=(\mathcal{F}_t)_{t=0,1,\dots,T}$ is an increasing family of sub-sigma-algebras of $\mathcal{F}$ \cite[Definition 5.1]{FS16}.

We assume the following (deal-level) data generating process for PEF cash flows.
The observed cash flows can be described by a vector of contribution and distribution pairs $CF = \left\{ (C_1, D_1), (C_2, D_2), \dots, (C_N, D_N) \right\}$.
\textcolor{darkgreen}{The contribution $C_i$ represents the investment cash flow paid to purchase the $i$th asset, and the distribution $D_i$ is the corresponding divestment cash flow received when the $i$th asset is sold again.}
We generate the contributions by $C_i = X_i \prod_{t=0}^{c_i} R_t e_{i,t}$ and define the corresponding distribution as $D_i=-C_i \prod_{t=c_i+1}^{d_i} R_t e_{i,t}$ where $c_i$ denotes the time of contributions, $d_i$ is the time of the distribution, $R_t$ is the (systematic) return in period $t$, and $e_{i,t}$ is the idiosyncratic error term of the $i$th \textcolor{darkgreen}{asset} in period $t$.
The stochastic process for the (market) return $R$ shall exhibit no serial dependence, i.e., $\mathbb{E} \left[ R_{t} R_{t+1} \right] = \mathbb{E} \left[ R_{t} \right] \mathbb{E} \left[ R_{t+1} \right] \ \forall \ t$.
Clearly, $i=1,2,\dots,N$.
Since needed later, we define the vectors of investment and divestment dates $c = \left\{ c_1, c_2, \dots, c_N \right\}$ and $d = \left\{ d_1, d_2, \dots, d_N \right\}$, respectively.
The random variable for the \textcolor{darkgreen}{arbitrary} start value $X_i$ needs no further specification.
We assume the (multiplicative) error terms $e$ to be i.i.d.\ with $\mathbb{E} \left[ e_{i,t} \right]=1$, \textcolor{darkgreen}{$\mathbb{E} \left[ e_{i,t} R_{t} \right] = \mathbb{E} \left[R_{t} \right]$}, and $e_{i,t} \geq 0$ for all $i,t$.
The systematic return is also i.i.d.\ with $\mathbb{E} \left[ R_{i,t} \right]>1$, however, and $R_{i,t} \geq 0$ for all $i,t$.
The entry shall occur before the exit $0 < c_i < d_i < T$ for all investments $i$.

\begin{remark}
	This deal-level data generating process produces gross-of-fee cash flows.
	Here our definition of the distribution variable $D$ is basically taken from Equation \ref{eq:data_generating_process} which originates from the \cite{ACGP18} paper.
	In Chapter \ref{chap:spatial_sdf_estimator} and \ref{chap:quadratic_hedging} we empirically use fund-level net-of-fee cash flows.
	However, in Chapter \ref{chap:modeling_the_exit_cash_flows} we describe a marked point process on deal level that explicitly can handle fee cash flows and can be seen as an extension of the above DGP. 
\end{remark}

Next, we specify our trivial inverse SDF, which uses a similar idea to the one from Equation \ref{eq:trivial_sdf}.
Our new SDF reverts the data generation process; Equation \ref{eq:trivial_sdf} inverts the empirically observed returns.

\begin{assume}
	\label{ass:trivial_inverse_sdf}
	Trivial inverse SDF and basic pricing relation: \\
	We define our SDF in terms of the systematic return that is relevant for all $N$ fund investments
	\begin{equation}
		\label{eq:trivial_inverse_sdf}
		\Psi_{u,v} = \frac{\prod_{t=0}^{u} R_t}{\prod_{t=0}^{v} R_t}
	\end{equation}
\end{assume}

This choice of SDF indicates that we want to rather identify the underlying data generating process (or better return generating process) than estimate a universal pricing kernel that is valid for all market cash flows.

For our SDF, the following basic pricing relation holds using the multi-period Riesz representation Theorem \ref{theo:riesz_multi_period}.
Using our particular design for the DGP and SDF, we can show that we use our SDF from Equation \ref{eq:trivial_inverse_sdf} not just for NPV but also for NFV calculations with $\tau>0$ in the following formula.

\begin{proposition}
	\label{prop:basic_pricing_relation}
	Basic pricing relation:
	\[
	\pi_{\tau} \left( CF \right) =
	\mathbb{E} \left[ 
	\sum_i^N
	\left(
	\Psi_{\tau,c_i} C_i + \Psi_{\tau,d_i} D_i 
	\right)
	\left| \mathcal{F}_{0} \right.
	\right] = 0
	\quad \forall \ \tau \in \left[0,1, \dots, T \right]
	\]
\end{proposition}


\begin{proof}
	First let us write out and simplify the discounted contribution term
	\[
	\Psi_{0,c_i} C_i =
	\frac{1}{\prod_{t=0}^{c_i} R_t} X_i \prod_{t=0}^{c_i} R_t e_{i,t} =
	X_i \prod_{t=0}^{c_i} e_{i,t}
	\]
	and discounted distribution term
	\[
	\Psi_{0,d_i} D_i = \frac{1}{\prod_{t=0}^{d_i} R_t} \left( - X_i \prod_{t=0}^{c_i} R_t e_{i,t} \right) \prod_{t=c_i+1}^{d_i} R_t e_{i,t} = - X_i \prod_{t=0}^{d_i} e_{i,t}
	\]
	When we plug these two expressions in the basic pricing relation, we can easily see that
	\[
	\pi_0 \left( CF \right) =
	\mathbb{E} \left[ 
	\sum_i^N
	\left(
	X_i \prod_{t=0}^{c_i} e_{i,t} - X_i \prod_{t=0}^{d_i} e_{i,t}
	\right)
	\left| \mathcal{F}_{0} \right.
	\right] = 0
	\]
	which follows directly from the definition of the i.i.d. error term $e_{i,t}$.
	
	Similarly, we can write \textcolor{darkgreen}{for a general $\tau$ (instead of having $\tau=0$)}
	\[
	\Psi_{\tau,c_i} C_i =
	X_i \prod_{t=0}^{\tau} R_t \prod_{t=0}^{c_i} e_{i,t}
	\]
	and
	\[
	\Psi_{\tau,d_i} D_i = 
	- X_i \prod_{t=0}^{\tau} R_t \prod_{t=0}^{d_i} e_{i,t}
	\]
	Thus, \textcolor{darkgreen}{we use the assumed zero correlation between $R_t$ and $e_{i,t}$ for all $i,t$ to obtain}
	\[
	\mathbb{E} \left[ 
	\sum_i^N
	\left(
	\Psi_{\tau,c_i} C_i + \Psi_{\tau,d_i} D_i
	\right)
	\left| \mathcal{F}_{0} \right.
	\right] = 
	\mathbb{E} \left[ 
	\sum_i^N
	\left(
	X_i \prod_{t=0}^{\tau} R_t
	\left(
	\prod_{t=0}^{c_i} e_{i,t} - \prod_{t=0}^{d_i} e_{i,t}
	\right)
	\right)
	\left| \mathcal{F}_{0} \right.
	\right] = 0
	\]
	which yields the general result for all $\tau$ since $\mathbb{E} \left[ \prod_{t=0}^{c_i} e_{i,t} - \prod_{t=0}^{d_i} e_{i,t} \left| \mathcal{F}_{0} \right. \right]=0$.
	$\square$
\end{proof}


\begin{remark}
	\cite{GSW19} note that the error terms, e.g., in expressions like $\mathbb{E} \left[ \prod_{t=0}^{c_i} e_{i,t} - \prod_{t=0}^{d_i} e_{i,t} \right]=0$, will not exactly cancel out in empirical datasets which causes their so-called "compounding bias" in finite samples.
\end{remark}



\subsubsection{Expectation of the pricing error $\epsilon$}


In this paragraph, we analyze under which conditions the expectation of the sum of all discounted cash flows is zero.
Only cases where this expectation is \textcolor{darkgreen}{deterministic (e.g., exactly zero)} are suitable candidates for moment conditions in GMM or extremum estimation.
Importantly, we define the term pricing error $\epsilon$ as the sum of all discounted net cash flows (and not as some error term in, e.g., a return factor model).

\iffalse

In the first lemma, we show that we can use the trivial inverse SDF to price a cash flow pair.

\begin{lemma}
	\label{lemma:single_cash_flow_price}
	With the SDF from Assumption \ref{ass:trivial_inverse_sdf} for each $i$  it holds 
	\[
	\mathbb{E} \left[ C_i + \Psi_{c_i,d_i} D_i |  \mathcal{F}_{t} \right] = 0
	\]
	just if $0 \leq t \leq c_i$.
\end{lemma}

\begin{proof}
	First, we analyze the case $\mathcal{F}_{t}$ with $0 \leq t \leq c_i$.
	Here we know
	\[
	\mathbb{E} \left[ \Psi_{c_i,d_i} D_i \right] = 
	\mathbb{E} \left[ \frac{\prod_{t=0}^{c_i} R_t}{\prod_{t=0}^{d_i} R_t} \left( -C_i \right) \prod_{t=c_i+1}^{d_i} R_t e_{i,t}  \right] =
	\mathbb{E} \left[ -C_i \prod_{t=c_i+1}^{d_i} \left( \frac{R_t}{R_t} e_{i,t} \right) \right]
	\]
	Thus
	\[
	\mathbb{E} \left[ \Psi_{c_i,d_i} D_i \right]=\mathbb{E} \left[ -C_i \prod_{t=c_i+1}^{d_i} e_{i,t} \right]= \mathbb{E} \left[ -C_i \right] \mathbb{E} \left[ \prod_{t=c_i+1}^{d_i} e_{i,t} \right] + \mathrm{Cov} \left[ -C_i, \prod_{t=c_i+1}^{d_i} e_{i,t} \right]
	\] 
	with $\mathbb{E} \left[ \prod_{t=c_i+1}^{d_i} e_{i,t} \right] = 1$ and $\mathrm{Cov} \left[ -C_i, \prod_{t=c_i+1}^{d_i} e_{i,t} \right]=0$.
	This yields $\mathbb{E} \left[ \Psi_{c_i,d_i} D_i \right]=-\mathbb{E} \left[ C_i \right]$ and consequentially $\mathbb{E} \left[ C_i + \Psi_{c_i,d_i} D_i  \right] = 0$. 
	
	Second, we analyze the case $\mathcal{F}_{t}$ with $t > c_i$.
	For simplicity, we choose $\mathcal{F}_{c_i+1}$ to receive
	\[
	\mathbb{E} \left[ C_i + \Psi_{c_i,d_i} D_i | \mathcal{F}_{c_i+1} \right] =
	C_i + \mathbb{E} \left[ \Psi_{c_i,d_i} D_i | \mathcal{F}_{c_i+1} \right]
	\]
	where we can split the SDF in a realized and unrealized component and plug-in the expression for $D_i$
	\[
	C_i + \mathbb{E} \left[ \Psi_{c_i,c_i+1} \Psi_{c_i+1,d_i} \left( -C_i \prod_{t=c_i+1}^{d_i} R_t e_{i,t} \right) | \mathcal{F}_{c_i+1} \right]
	\]
	Next we factor out the already realized terms
	\[
	C_i + \Psi_{c_i,c_i+1} R_{c_i+1} e_{i,c_i+1}
	\mathbb{E} \left[  \Psi_{c_i+1,d_i} \left( -C_i \prod_{t=c_i+2}^{d_i} R_t e_{i,t} \right) | \mathcal{F}_{c_i+1} \right]
	\]
	which yields
	\[
	C_i + e_{i,c_i+1}
	\mathbb{E} \left[  -C_i \prod_{t=c_i+2}^{d_i} e_{i,t} | \mathcal{F}_{c_i+1} \right]
	=
	C_i - C_i e_{i,c_i+1}
	\]
	that is obviously just zero for the special case $e_{i,c_i+1}=1$.
	$\square$
\end{proof}

As expected, Lemma \ref{lemma:single_cash_flow_price} demonstrates that we can use the trivial SDF from Assumption \ref{ass:trivial_inverse_sdf} to correctly price a single cash flow pair $(C_i, D_i)$.

Next, we want to show that we can use the same SDF to discount a cash flow stream to multiple points in time without changing the price (which is zero in cases under consideration).

\begin{proposition}
	\label{prop:zero_expectation_at_the_end}
	Using the SDF from Assumption \ref{ass:trivial_inverse_sdf} it holds
	\begin{enumerate}
		\item 
		$\sum_i^N \mathbb{E} \left[ \Psi_{\tau,c_i} C_i + \Psi_{\tau,d_i} D_i | \mathcal{F}_{c_i}  \right] = 0 \ $
		for all $\tau$,
		\item 
		$\sum_i^N \mathbb{E} \left[ \Psi_{\tau,c_i} C_i + \Psi_{\tau,d_i} D_i | \mathcal{F}_{0}  \right] = 0 \ $
		if $ \ \tau > c_i$ for each cash flow pair $i$.
	\end{enumerate}
\end{proposition}


\begin{proof}
	Again we generally assume all expectations use $\mathcal{F}_{t}$-information with $0 \leq t \leq c_i$.
	Then we know
	\[
	\mathbb{E} \left[ \Psi_{\tau,d_i} C_i + \Psi_{\tau,d_i} D_i  \right] = 
	\mathbb{E} \left[ 
	\frac{\prod_{t=0}^{\tau} R_t }{\prod_{t=0}^{c_i} R_t} C_i + \frac{\prod_{t=0}^{\tau} R_t }{\prod_{t=0}^{d_i} R_t} D_i \right]
	\]
	Here we can split the second SDF term into two parts
	\[
	\mathbb{E} \left[ \frac{\prod_{t=0}^{\tau} R_t }{\prod_{t=0}^{c_i} R_t} C_i + \frac{\prod_{t=0}^{\tau} R_t }{\prod_{t=0}^{c_i} R_t} \cdot \frac{\prod_{t=0}^{c_i} R_t }{\prod_{t=0}^{d_i} R_t} D_i \right] = 
	\mathbb{E} \left[ \frac{\prod_{t=0}^{\tau} R_t }{\prod_{t=0}^{c_i} R_t} \left( C_i + \frac{\prod_{t=0}^{c_i} R_t }{\prod_{t=0}^{d_i} R_t} D_i \right) \right]
	\]
	This yields
	\[
	\mathbb{E} \left[ \Psi_{\tau,c_i} \left( C_i + \Psi_{c_i,d_i} D_i \right) \right] = \mathbb{E} \left[ \Psi_{\tau,c_i} \right] \mathbb{E} \left[ C_i + \Psi_{c_i,d_i} D_i \right] + \mathrm{Cov} \left[ \Psi_{\tau,c_i}, \left( C_i + \Psi_{c_i,d_i} D_i \right) \right]
	\]
	where the first summand is zero due to Lemma \ref{lemma:single_cash_flow_price}.
	Hence we focus on the covariance term, which we can write as
	\[
	\mathrm{Cov} \left[ \Psi_{\tau,c_i}, \left( C_i + \frac{\prod_{t=0}^{c_i} R_t }{\prod_{t=0}^{d_i} R_t} \left( -C_i \prod_{t=c_i+1}^{d_i} R_t e_{i,t} \right) \right) \right]
	=
	\mathrm{Cov} \left[ \Psi_{\tau,c_i}, C_i \left( 1 - \prod_{t=c_i+1}^{d_i} e_{i,t} \right) \right]
	\]
	by plugging in the corresponding definitions of $\Psi_{c_i,d_i}$ and $D_i$.
	Next we need to analyze $\mathrm{Cov} \left[ \Psi_{\tau,c_i}, C_i \right]$ which we can write as
	\[
	\mathrm{Cov} \left[ \Psi_{\tau,c_i}, C_i \right] =
	\mathrm{Cov} \left[  \frac{\prod_{t=0}^{\tau} R_t }{\prod_{t=0}^{c_i} R_t}, X_i \prod_{t=0}^{c_i} R_t e_{i,t} \right]
	\]
	Now we can distinguish two cases
	\begin{enumerate}
		\item If we use $\mathcal{F}_{c_i}$ information, $C_i$ is an already realized random variable. Thus the covariance term becomes zero regardless of $\tau$.
		\item If we use $\mathcal{F}_{0}$ information, the covariance term becomes zero just for $\tau > c_i$ (or for the unlikely scenario that $\prod_{t=c_i+1}^{d_i} e_{i,t}=1$).
	\end{enumerate}
	For these two cases, each summand of the expression $\sum_i^N \mathbb{E} \left[ \Psi_{\tau,d_i} C_i + \Psi_{\tau,d_i} D_i  \right]$ is equal to zero. 
	$\square$
\end{proof}

\fi


\begin{definition}
	\label{def:pricing_error}
	Pricing error: \\
	The fund pricing error at time $\tau$ is defined as
	\[
	\epsilon_{\tau} = \sum_{i=1}^N \left( \Psi_{\tau,c_i} C_i + \Psi_{\tau,d_i} D_i \right)
	\]
	where $N$ gives the number of underlying deals in the PE fund.
\end{definition}


The next corollary characterizes the set of dates to which we can "safely" discount without receiving a nonzero expectation for the fund pricing error at time $\tau$.


\begin{corollary}
	\label{coro:expected_epsillion}
	From Proposition \ref{prop:basic_pricing_relation} it follows
	\[
	\mathbb{E} \left[ \sum_{\tau \in \mathcal{T}} \epsilon_{\tau} | \mathcal{F}_{0}  \right] = 0
	\]
	where the set of discount dates is $\mathcal{T} \subseteq \left\{0, 1, 2, \dots, T \right\}$.
\end{corollary}


\begin{proof}
	The proof directly follows from Proposition \ref{prop:basic_pricing_relation} \textcolor{darkgreen}{that gives us $\mathbb{E} \left[ \epsilon_{\tau} | \mathcal{F}_{0}  \right] = 0$ for all $\tau \in \mathcal{T}$}.
	$\square$
\end{proof}


\subsubsection{Variance of the pricing error $\epsilon$}


Next, we want to focus on the variance of $\epsilon_{\tau}$ instead of its expected value.
\textcolor{darkgreen}{
	This helps us better understand the loss function variance in Chapter \ref{chap:spatial_sdf_estimator}, where we want to find SDF parameters that minimize the empirical pricing errors $\epsilon$.
}

\begin{lemma}
	\label{lemma:variance_epsilon}
	Assume a fund with just one cash flow $N=1$ and $c_1 > 0$ and $d_1 < T$.
	Then $\mathrm{Var} \left[ \epsilon_{\tau} \right] < \mathrm{Var} \left[ \epsilon_{\tau+1} \right]$ for each $\tau \in \left\{ 0, 1, \dots, T-1 \right\}$.
\end{lemma}

\begin{proof}
	Since we just have $i=1$, we drop the the $i$ subscript in this proof.
	Additionally, we use the follow notation $R_{a,b} = \frac{\prod_{t=0}^{a} R_t}{\prod_{t=0}^{b} R_t}$, $e_{a,b} = \frac{\prod_{t=0}^{a} e_t}{\prod_{t=0}^{b} e_t}$.
	We can express the variance term as $\mathrm{Var} \left[ \epsilon_{\tau} \right] = \mathrm{Var} \left[ \Psi_{\tau,c} \left( C + \Psi_{c,d} D \right) \right]$
	which equals
	\[
	\mathrm{Var} \left[ \Psi_{\tau,c} C \right] +
	\mathrm{Var} \left[ \Psi_{\tau,c} \Psi_{c,d} D \right] +
	2 \cdot \mathrm{Cov} \left[ \Psi_{\tau,c} C, \Psi_{\tau,c} \Psi_{c,d} D \right]
	\]
	or equally when plugging in the definition of the distribution variable $D$
	\[
	\mathrm{Var} \left[ \Psi_{\tau,c} C \right] +
	\mathrm{Var} \left[ \Psi_{\tau,c} \Psi_{c,d} (-C)  R_{d,c} e_{d,c} \right] +
	2 \cdot \mathrm{Cov} \left[ \Psi_{\tau,c} C, \Psi_{\tau,c} \Psi_{c,d} (-C) R_{d,c} e_{d,c} \right]
	\]
	Next, we rewrite the SDFs as returns
	\[
	\mathrm{Var} \left[ R_{\tau,c} C \right] +
	\mathrm{Var} \left[ R_{\tau,c} R_{c,d} (-C)  R_{d,c} e_{d,c} \right] +
	2 \cdot \mathrm{Cov} \left[ R_{\tau,c} C, R_{\tau,c} R_{c,d} (-C) R_{d,c} e_{d,c} \right]
	\]
	which we can simplify to
	\[
	\mathrm{Var} \left[ R_{\tau,c} C \right] +
	\mathrm{Var} \left[ R_{\tau,c} (-C) e_{d,c} \right] +
	2 \cdot \mathrm{Cov} \left[ R_{\tau,c} C, R_{\tau,c} (-C) e_{d,c} \right]
	\]
	Further we can use the fact that $- e_{d,c}$ is independent from $R_{\tau,c} C$ to obtain
	\[
	\mathrm{Var} \left[ R_{\tau,c} C \right] +
	\mathrm{Var} \left[ R_{\tau,c} (-C) e_{d,c} \right] +
	2 \cdot \mathrm{Var} \left[ R_{\tau,c} C \right]
	\mathbb{E} \left[ - e_{d,c} \right]
	\]
	and finally recognize that $\mathbb{E} \left[ - e_{d,c} \right] = -1$
	\[
	\mathrm{Var} \left[ R_{\tau,c} (-C) e_{d,c} \right] -
	\mathrm{Var} \left[ R_{\tau,c} C \right]
	\]
	Here we can plug in the definition of $C=X R_{c, 0} e_{c, 0}$
	\[
	\mathrm{Var} \left[ R_{\tau,c} (-X R_{c, 0} e_{c, 0}) e_{d,c} \right] -
	\mathrm{Var} \left[ R_{\tau,c} X R_{c, 0} e_{c, 0} \right]
	=
	\mathrm{Var} \left[ R_{\tau,0} (-X ) e_{d,0} \right] -
	\mathrm{Var} \left[ R_{\tau,0} X e_{c, 0} \right]
	\]
	which we can regroup to
	\[
	\mathrm{Var} \left[ - e_{d,c} \left( R_{\tau,0} X e_{c, 0} \right)  \right] -
	\mathrm{Var} \left[ R_{\tau,0} X e_{c, 0} \right]
	\]
	Next, we rewrite the variance in terms of expectations (since the error term $- e_{d,c}$ is independent from $R_{\tau,0} X e_{c, 0}$)
	\[
	\left\{
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right)^2 \right]
	\mathbb{E} \left[ e_{d,c}^2 \right]
	-
	\left(
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right) \right]
	\mathbb{E} \left[ - e_{d,c} \right]
	\right)^2
	\right\}
	-
	\left\{
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right)^2 \right]
	-
	\left(
	\mathbb{E} \left[ R_{\tau,0} X e_{c, 0} \right]
	\right)^2
	\right\}
	\]
	which we can reformulate to
	\[
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right)^2 \right]
	\left\{
	\mathbb{E} \left[ e_{d,c}^2 \right]
	- 1
	\right\}
	-
	\left(
	\mathbb{E} \left[ R_{\tau,0} X e_{c, 0} \right]
	\right)^2
	\left\{
	\left(
	\mathbb{E} \left[ - e_{d,c} \right]
	\right)^2
	- 1
	\right\}
	\]
	Since we know $\left( \mathbb{E} \left[ - e_{d,c} \right] \right)^2 = 1$, we can drop the second term to just receive
	\[
	\mathrm{Var} \left[ \epsilon_{\tau} \right] = 
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right)^2 \right]
	\left\{
	\mathbb{E} \left[ e_{d,c}^2 \right]
	- 1
	\right\}
	\]
	Here, we remark that $\mathbb{E} \left[ e_{d,c}^2 \right] \geq 1$ since always $\mathbb{E} \left[ e_{d,c}^2 \right] \geq \left( \mathbb{E} \left[ e_{d,c} \right] \right)^2$.
	So we just have to show that 
	\[
	\mathbb{E} \left[ \left( R_{\tau+1,0} X e_{c, 0} \right)^2 \right] > 
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right)^2 \right] \qquad \forall \ \tau
	\]
	Here we use the fact that $R_{\tau+1,0} = R_{\tau+1} \cdot R_{\tau,0}$ to rewrite the first term as
	\[
	\mathbb{E} \left[ \left( R_{\tau+1,0} X e_{c, 0} \right)^2 \right]
	=
	\mathbb{E} \left[ R_{\tau+1}^2 \left( R_{\tau,0} X e_{c, 0} \right)^2 \right]
	=
	\mathbb{E} \left[ R_{\tau+1}^2 \right]
	\mathbb{E} \left[ \left( R_{\tau,0} X e_{c, 0} \right)^2 \right]
	\]
	with $\mathbb{E} \left[ R_{\tau+1}^2 \right] \geq \left( \mathbb{E} \left[ R_{\tau+1} \right] \right)^2 > 1$ by our definition of $R$.
	This concludes the proof.
	$\square$
\end{proof}

Next, we want to bound the variance for sums of $\epsilon_{\tau}$'s.

\begin{proposition}
	\label{prop:variance_sum_epsilons}
	In the setting of Lemma \ref{lemma:variance_epsilon} it holds for the variances of $\epsilon_{\tau}$ 
	\[
	\mathrm{Var} \left[ \epsilon_{\tau=0} \right] <
	\mathrm{Var} \left[ \frac{1}{T+1} \sum_{\tau=0}^T \epsilon_{\tau} \right] < 
	\mathrm{Var} \left[ \epsilon_{\tau=T} \right]
	\]
	with $T>0$.
\end{proposition}


\begin{proof}
	Since the $\epsilon_{\tau}$'s are potentially autocorrelated, we have to first check if there is serial dependence in the series $\left\{ \epsilon_{\tau=0}, \epsilon_{\tau=1}, \dots  \right\}$.
	Specifically, we analyze
	\[
	\mathrm{Cov} \left[  \epsilon_{\tau=u}, \epsilon_{\tau=v} \right]
	=
	\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{u} \Psi_{v,u} \right]
	\]
	which leads to the question about the dependence between $\epsilon_{u}$ and $\Psi_{v,u}$.
	Here we know by definition
	\[
	\epsilon_{u} 
	= 
	\sum_{i=1}^N \left( \Psi_{u,c_i} C_i + \Psi_{u,d_i} D_i \right)
	=
	\sum_{i=1}^N 
	\left[
	R_{u,c_i} \left( X_i R_{c_i,0} e_{c_i,0,i} \right) - R_{u,d_i} \left( X_i R_{c_i,0} e_{c_i,0,i} \right) R_{d_i,c_i} e_{d_i,c_i,i}
	\right]
	\]
	which we can simplify to
	\[
	\sum_{i=1}^N \left( R_{u,0} X_i e_{c_i,0,i} - R_{u,0} X_i e_{d_i,0,i} \right)
	=
	R_{u,0} \sum_{i=1}^N X_i \left( e_{c_i,0,i} - e_{d_i,0,i} \right)
	\]
	Thus
	\[
	\epsilon_{u} \Psi_{v,u} =
	R_{u,0} \sum_{i=1}^N X_i \left( e_{c_i,0,i} - e_{d_i,0,i} \right) R_{v,u} =
	R_{v,0} \sum_{i=1}^N X_i \left( e_{c_i,0,i} - e_{d_i,0,i} \right)
	\]
	From this formulation, we can see that (i) $\epsilon_{u}$ and $\Psi_{v,u}$ are negatively correlated, and (ii) the positive correlation between $\epsilon_{u}$ and $\epsilon_{v}$ decreases with the time difference between the two dates $| u - v |$.
	To further ease the notation, we substitute $W = \sum_{i=1}^N X_i \left( e_{c_i,0,i} - e_{d_i,0,i} \right)$ to obtain
	\[
	\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] =
	\mathrm{Cov} \left[ R_{u,0} W, R_{v,0} W \right] =
	\mathrm{Cov} \left[ R_{u,0} W, R_{v,u} R_{u,0} W \right]
	\]
	which yields $\mathrm{Var} \left[  \epsilon_{0} \right] = \mathrm{Var} \left[  W \right]$.
	For $u<v$ we utilize the independence between $R_{v,u}$ and $R_{u,0} W$ to receive
	\[
	\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] =
	\mathrm{Var} \left[ R_{u,0} W \right] \mathbb{E} \left[ R_{v,u} \right]
	\]
	and for $u>v$
	\[
	\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] =
	\mathrm{Var} \left[ R_{v,0} W \right] \mathbb{E} \left[ R_{u,v} \right]
	\]
	or generally
	\[
	\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] =
	\mathrm{Var} \left[ R_{\min(u,v),0} W \right] \mathbb{E} \left[ R_{\max(u,v),\min(u,v)} \right] =
	\mathrm{Var} \left[ \epsilon_{\min(u,v)} \right] \mathbb{E} \left[ \Psi_{\max(u,v),\min(u,v)} \right]
	\]
	Thus we obtain for
	\[
	\sum_{u=0}^T \sum_{v=0}^T \mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] =
	\sum_{u=0}^T \mathrm{Var} \left[  \epsilon_{u} \right] + 
	2 \sum_{u<v} \mathrm{Var} \left[ \epsilon_{u} \right] \mathbb{E} \left[ \Psi_{v,u} \right]
	\]
	Further it obviously holds
	\[
	\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] =
	\mathrm{Corr} \left[  \epsilon_{u}, \epsilon_{u} \Psi_{v,u} \right]
	\sqrt{\mathrm{Var} \left[  \epsilon_{u} \right]}
	\sqrt{\mathrm{Var} \left[  \epsilon_{u} \Psi_{v,u} \right]}
	< 
	\mathrm{Var} \left[  \epsilon_{\max \left( u, v \right)} \right]
	\]
	Next, we use the results we obtained so far to calculate the total variance
	\[
	\mathrm{Var} \left[ \frac{1}{T+1} \sum_{\tau=0}^T \epsilon_{\tau} \right]
	=
	\left( \frac{1}{T+1} \right)^2 	
	\mathrm{Var} \left[ \sum_{\tau=0}^T \epsilon_{\tau} \right]
	=
	\left( \frac{1}{T+1} \right)^2 
	\left( 
	\sum_{u=0}^T \sum_{v=0}^T \mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right]
	\right)
	\]
	which is in turn
	\[
	\mathrm{Var} \left[ \frac{1}{T+1} \sum_{\tau=0}^T \epsilon_{\tau} \right]
	=
	\frac{1}{\left( T + 1 \right)^2}
	\left(
	\sum_{u=0}^T \mathrm{Var} \left[  \epsilon_{u} \right] + 
	2 \sum_{u<v} \mathrm{Var} \left[ \epsilon_{u} \right] \mathbb{E} \left[ \Psi_{v,u} \right]
	\right)
	\]
	For all $u,v$, we know $\mathrm{Cov} \left[  \epsilon_{0}, \epsilon_{0} \right] \leq \mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right] \leq \mathrm{Cov} \left[  \epsilon_{T}, \epsilon_{T} \right]$.
	Thus, we can conclude using Lemma \ref{lemma:variance_epsilon} (which implies $\mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{u} \right]<\mathrm{Cov} \left[  \epsilon_{u+1}, \epsilon_{u+1} \right]$)
	\[
	\left( T + 1 \right)^2
	\mathrm{Cov} \left[  \epsilon_{0}, \epsilon_{0} \right]
	<
	\sum_{u=0}^T \sum_{v=0}^T \mathrm{Cov} \left[  \epsilon_{u}, \epsilon_{v} \right]
	<
	\left( T + 1 \right)^2
	\mathrm{Cov} \left[  \epsilon_{T}, \epsilon_{T} \right]
	\]
	or equivalently
	\[
	\left( T + 1 \right)^2
	\mathrm{Var} \left[ \epsilon_{\tau=0} \right] <
	\left( T + 1 \right)^2
	\mathrm{Var} \left[ \frac{1}{T+1} \sum_{\tau=0}^T \epsilon_{\tau} \right] <
	\left( T + 1 \right)^2
	\mathrm{Var} \left[ \epsilon_{\tau=T} \right]
	\]
	which yields the stated proposition when we divide by $\left( T + 1 \right)^2$.
	$\square$
\end{proof}

In the following corollary, we generalize Proposition \ref{prop:variance_sum_epsilons} for all so-called $\mathcal{T}$-averaged pricing error \textcolor{darkgreen}{which will be used in Chapter \ref{chap:spatial_sdf_estimator}.}

\begin{definition}
	\label{def:tau_averaged_pricing_error}
	The $\mathcal{T}$-averaged pricing error is defined as
	\[
	\bar{\epsilon} = \frac{1}{\mathrm{card}(\mathcal{T})} \sum_{\tau \in \mathcal{T}} \epsilon_{\tau}
	\]
	where $\mathcal{T} \subseteq \left\{0,1,2,\dots,T \right\}$ and $\mathrm{card}(\mathcal{T})$ gives the cardinality of the set $\mathcal{T}$.
\end{definition}


\begin{corollary}
	\label{coro:epsilon_variance_bounds}
	From Proposition \ref{prop:variance_sum_epsilons} it follows more generally
	\[
	\mathrm{Var} \left[ \epsilon_{\tau=0} \right] \leq
	\mathrm{Var} \left[ \bar{\epsilon} \right] \leq
	\mathrm{Var} \left[ \epsilon_{\tau=T} \right]
	\]
	where the $\mathcal{T}$-averaged pricing error $\bar{\epsilon}$ is defined by Definition \ref{def:tau_averaged_pricing_error}.
\end{corollary}

\begin{proof}
	The proof is straightforward and follows the same steps as the proof for Proposition \ref{prop:variance_sum_epsilons}. $\square$
\end{proof}

\textcolor{darkgreen}{
	In Corollary \ref{coro:epsilon_variance_bounds}, $\mathcal{T}$ can be any subset of $\left\{0,1,2,\dots,T \right\}$ that contains more than just the element zero.
	In the special case $\mathcal{T} = \left\{0\right\}$, we have $\bar{\epsilon} = \epsilon_{\tau=0}$ obviously.
}


\subsection{Newton-type methods for nonlinear optimization}
\label{subsec:newton_methods_nlo}

Now we assume a simple linear return factor model $R = 1 + \beta \cdot \left( R_m - 1\right) = 1 + \beta R_m - \beta$ with scalar $\beta$, and $R_m$ can be interpreted as excess market return (with a zero risk-free rate).
W.l.o.g., we choose here the simplest of all factor models to not clutter the following expressions.
However, the general insights from this section also hold for factor models like $R = \alpha + R_{rf} + \beta^{\top} F$ with a vector $\beta$.

\begin{definition}
	\label{def:simple_linear_sdf}
	Simple linear SDF: \\
	The simple linear SDF is defined as
	\begin{equation}
		\label{eq:simple_linear_sdf}
		\Psi_{u,v}^{\mathrm{SL}} = 
		\frac{\prod_{t=0}^{u} \left( 1 +  \beta \cdot \left( R_{m,t} - 1\right)  \right)}{\prod_{t=0}^{v} \left( 1 + \beta \cdot \left( R_{m,t} - 1\right)  \right)}
	\end{equation}
	where $R_{m,t}$ is now the market return in period $t$ (and \textbf{not} a multi-period return between $m$ and $t$).
\end{definition}

Let us assume we can just observe the cash flows $CF$ of several funds, and the excess market returns $R_m$.
Then we analyze the following \textbf{nonlinear} minimization task (which is an extremum estimator according to Definition \ref{def:extremum_estimator})
\[
\beta^* = \arg \min_{\beta} f(\beta)
\]
with 
\[
f(\beta) = \sum_{j=1}^J L \left( \bar{\epsilon}_{j} \right)
\]
where
\[
\bar{\epsilon}_{j} =
\frac{1}{\mathrm{card}(\mathcal{T})} \sum_{\tau \in \mathcal{T}} \epsilon_{\tau,j}
=
\frac{1}{\mathrm{card}(\mathcal{T})} \sum_{\tau \in \mathcal{T}}
\sum_{i=1}^N \left( \Psi_{\tau,c_{i,j}}^{\mathrm{SL}} C_{i,j} + \Psi_{\tau,d_{i,j}}^{\mathrm{SL}} D_{i,j} \right)
\]
is the $\mathcal{T}$-averaged pricing error for the $j$th cross-sectional unit (i.e., a PE fund in our case).
The pricing error $\epsilon_{\tau,j}$ follows Definition \ref{def:pricing_error} and uses the SDF from Definition \ref{def:simple_linear_sdf}.
Here $L$ is a loss function; for simplicity, we assume the case of nonlinear least-squares with $L(x)=x^2$.
\textcolor{darkgreen}{
	For the empirical estimation of $\beta^*$, we need to consider $C_{i,j}$, $D_{i,j}$, and $R_{m,t}$ as realized random variables (empirical observations).
}

In contrast to linear problems (that can be often solved analytically by linear algebra), any nontrivial nonlinear system can usually just be solved by numerical methods.
Thus, in remainder of this subsection, we show that we can use so-called Quasi-Newton methods for estimating the parameter $\beta^*$ in our nonlinear optimization problem.
The pure Newton method or the Gauss-Newton algorithm is not applicable since the Gradient/Jacobian and Hessian terms \textcolor{darkgreen}{might be often} too cumbersome in our general case \textcolor{darkgreen}{since the numerator and denominator of the SDF, given by Equation \ref{eq:simple_linear_sdf}, can contain high-order polynomials of $\beta$.}

\iffalse
\textcolor{orange}{
	\begin{exercise}
		Q: Is there theoretically a closed-form solution that is just also too cumbersome to calculate? \\
		A: In contrast to linear problems (that can be often solved analytically by linear algebra), any nontrivial nonlinear system can usually just be solved by numerical methods.
		% Compare to pseudo inverse for \textbf{linear (!)} OLS regression?
		% https://towardsdatascience.com/closed-form-solution-to-linear-regression-e1fe14c1cbef
		% Is matrix inversion too inefficient compared to "other solutions"?
		% Mathematicians know that you should "never invert a matrix"...
		% https://gregorygundersen.com/blog/2020/12/09/matrix-inversion/
	\end{exercise}
}
\fi

\subsubsection{Asymptotic analysis of $f(\beta)$ for large $\beta$}


First, let us establish a very useful result for the limiting behavior of $f(\beta)$.
Here we can utilize the following auxiliary results for an SDF.


\begin{lemma}
	\label{lem:limit_simple_linear_sdf}
	With Definition \ref{def:simple_linear_sdf}, it holds
	\begin{enumerate}
		\item $	\lim_{\beta \to \pm \infty} \ \Psi_{u, v}^{\mathrm{SL}}  = 0 \quad \ \ \ $ when $u < v$,
		\item $	\lim_{\beta \to \pm \infty} \ \Psi_{u, v}^{\mathrm{SL}} = \pm \infty \quad$ when $u > v$.
	\end{enumerate}
	if $R_{m,t} \neq  1$ for at least one $t$.
\end{lemma}


\begin{proof}
	By the SDF Definition \ref{def:simple_linear_sdf}, we know for $u<v$
	\[
	\Psi_{u,v}^{\mathrm{SL}} = \frac{1}{\prod_{t=u+1}^{v} \left( 1 + \beta \cdot \left( R_{m,t} - 1\right)  \right)}
	\]
	which tends to zero for large (positive or negative) $\beta$ since the degree of the polynomial in the denominator is obviously larger than the degree of the numerator
	\[
	\lim_{\beta \to \pm \infty} 
	\frac{1}{\prod_{t=u+1}^{v} \left( 1 + \beta \cdot \left( R_{m,t} - 1\right)  \right)} = 0
	\]
	which yields the first statement.
	
	In contrast, for $u>v$ the SDF is
	\[
	\Psi_{u,v}^{\mathrm{SL}} = \prod_{t=u+1}^{v} \left( 1 + \beta \cdot \left( R_{m,t} - 1\right)  \right)
	\]
	which goes to (plus or minus) infinity as $\beta$ grows extremely large or small
	\[
	\lim_{\beta \to \pm \infty} 
	\prod_{t=u+1}^{v} \left( 1 + \beta \cdot \left( R_{m,t} - 1\right)  \right) = \pm \ \infty
	\]
	which concludes the second part of the proof.
	$\square$
\end{proof}


The following proposition describes the limits of $f(\beta)$.
Here we can see that the standard SDF formula has its global minimum at $\beta = \pm \infty$, but when applying $\mathcal{T}$-averaging properly, we can obtain a finite global minimum.


\begin{proposition}
	\label{prop:exploding_beta}
	Exploding beta: \\
	Using a simple linear SDF $\Psi_{u, v}^{\mathrm{SL}}$ as given by Definition \ref{def:simple_linear_sdf}, it holds
	\begin{enumerate}
		\item $	\lim_{\beta \to \pm \infty} \ f(\beta) = 0 \quad \ $ for $\mathcal{T} = \left\{ 0 \right\}$,
		\item $	\lim_{\beta \to \pm \infty} \ f(\beta) = \infty \quad$ for $\mathcal{T} = \left\{ 0, x \right\}$ where $x \geq \min (d) $.
	\end{enumerate}
\end{proposition}


\begin{proof}
	We start with
	\[
	f(\beta) = \sum_{j=1}^J \left[
	\frac{1}{\mathrm{card}(\mathcal{T})} \sum_{\tau \in \mathcal{T}} 
	\sum_{i=1}^N \left( \Psi_{\tau, c_{i,j}}^{\mathrm{SL}} C_{i,j} + \Psi_{\tau,d_{i,j}}^{\mathrm{SL}} D_{i,j} \right)
	\right]^2
	\]
	\textcolor{darkgreen}{where $J$ gives the number of cross-sectional units (e.g., PE funds) and $N$ is the number of cash flow pairs.}
	For $\mathcal{T} = \left\{ 0 \right\}$, we have
	\[
	f(\beta) = 
	\sum_{j=1}^J \left[
	\sum_{i=1}^N \left( \Psi_{0, c_{i,j}}^{\mathrm{SL}} C_{i,j} + \Psi_{0,d_{i,j}}^{\mathrm{SL}} D_{i,j} \right)
	\right]^2
	\]
	By Lemma \ref{lem:limit_simple_linear_sdf}, we know that each $\Psi_{0, c_{i,j}}$ and $\Psi_{0, d_{i,j}}$ goes to zero as $\beta$ grows large.
	Thus each summand in the above equation goes to zero as $\beta$ approaches plus or negative infinity.
	This yields the proof for the first statement that $\lim_{\beta \to \pm \infty} \ f(\beta) = 0$ when $\mathcal{T} = \left\{ 0 \right\}$.
	
	For $\mathcal{T} = \left\{ 0, x \right\}$ with $x \geq \min (d)$, we obtain at least one summand of the form
	\[
	C \cdot \Psi_{d,c}^{\mathrm{SL}} = 
	C \left[ \prod_{t = c+1}^{d} \left( 1 + \beta \cdot \left( R_{m,t} - 1 \right) \right) \right]
	\]
	with $c < d$ which always approaches (positive or negative) infinity as $\beta$ goes to $\pm \ \infty$.
	In this case, all other terms (that could potentially all approach zero) become irrelevant and the whole function tends always to positive infinity.
	Since the loss function is quadratic it does not matter if $\Psi_{d,c}$ tends to positive or negative infinity; the quadratic term $\left( \Psi_{d,c} \right)^2$ always dominates.
	This concludes the proof for the second statement that $ \lim_{\beta \to \pm \infty} \ f(\beta) = \infty$ when $\mathcal{T} = \left\{ 0, x \right\}$ with $x \geq \min (d) $.
	$\square$
\end{proof}


\begin{corollary}
	\label{coro:exploding_alpha}
	Exploding alpha \cite[Footnote 8]{DLP12}: \\
	If the simple linear SDF additionally contains a constant term $\alpha$ , then
	\begin{enumerate}
		\item $	\lim_{\alpha \to \pm \infty} \ f(\alpha) = 0 \quad \ $ for $\mathcal{T} = \left\{ 0 \right\}$,
		\item $	\lim_{\alpha \to \pm \infty} \ f(\alpha) = \infty \quad$ for $\mathcal{T} = \left\{ 0, x \right\}$ where $x \geq \min (d) $.
	\end{enumerate}
	where the SDF is in this case $	\Psi_{u,v}^{\mathrm{SL}} = 
	\frac{\prod_{t=0}^{u} \left( 1 + \alpha +  \beta \cdot \left( R_{m,t} - 1\right)  \right)}{\prod_{t=0}^{v} \left( 1 + \alpha + \beta \cdot \left( R_{m,t} - 1\right)  \right)}$.
\end{corollary}


\begin{proof}
	The proof works by analogy to the proof of Proposition \ref{prop:exploding_beta}.
	$\square$
\end{proof}


\subsubsection{Identify irrelevant discounting dates in $\mathcal{T}$}


Next, we want to analyze which discounting dates in $\mathcal{T}$ can be considered irrelevant or unnecessary since "they provide no additional useful information to our estimation problem."
\textcolor{darkgreen}{
	In this subsection, we consider $f(\beta)$ as random function and, consequentially, all cash flows $C_{i,j}$, $D_{i,j}$ and market returns $R_{m,i}$ as random variables.
}

\begin{lemma}
	For a given fund $j$, the set $\mathcal{T}_j$ shall just include dates from the set of relevant cash flow dates which is $\left\{ \min (c_j) - 1, \dots, \max (d_j) \right\}$.
	Otherwise, if $\mathcal{T}_j \nsubseteq \left\{ \min (c_j) - 1, \dots, \max (d_j) \right\}$, squared SDF terms are introduced that increase the variance of $f(\beta)$.
\end{lemma}


\begin{proof}
	First, let us analyze the case $\mathcal{T}_j = \left\{ \max (d_j) + x \right\}$ for $x>0$.
	Here we have
	\[
	f(\beta) = \sum_{j=1}^J \left[
	\sum_{i=1}^N \left( \Psi_{\max (d_j) + x, c_{i,j}} C_{i,j} + \Psi_{\max (d_j) + x,d_{i,j}} D_{i,j} \right)
	\right]^2
	\]
	where all denominator components of the SDF cancel out for each summand.
	This is equivalent to
	\[
	f(\beta) = \sum_{j=1}^J \left[
	\Psi_{\max (d_j) + x, \max (d_j)}
	\sum_{i=1}^N \left( \Psi_{\max (d_j), c_{i,j}} C_{i,j} + \Psi_{\max (d_j),d_{i,j}} D_{i,j} \right)
	\right]^2
	\]
	which ultimately can be written as
	\[
	f(\beta) = 
	\sum_{j=1}^J \Psi_{\max (d_j) + x, \max (d_j)}^2
	\left[
	\sum_{i=1}^N \left( \Psi_{\max (d_j), c_{i,j}} C_{i,j} + \Psi_{\max (d_j),d_{i,j}} D_{i,j} \right)
	\right]^2
	\]
	So for $x > 0$, we have the minimization problem for $\mathcal{T} = \left\{ \max (d_j) \right\}$ times a squared scaling factor.
	
	Second, for dates before the fund inception $\mathcal{T} = \left\{ \min (c_j) - 1 - x \right\}$ with $x > 0$, we receive
	\[
	f(\beta) = 
	\sum_{j=1}^J \Psi_{\min (c_j) - 1, \min (c_j) - 1 - x}^2
	\left[
	\sum_{i=1}^N \left( \Psi_{\min (c_j) - 1, c_{i,j}} C_{i,j} + \Psi_{\min (c_j) - 1, d_{i,j}} D_{i,j} \right)
	\right]^2
	\]
	W.l.o.g., we can assume that the time index origin is located one period before the first fund cash flow, i.e., $\min (c) - 1 = 0$.
	For this "thought experiment," we have then to extend the time axis to the domain of negative integers $t \in \mathbb{R}$.
	Here we again can factor out a squared SDF term.
	
	Finally, we have to show why these squared terms are "unnecessary."
	Here it is easy to show that the function $f(\beta; \mathcal{T} = \left\{ \min (c) - 1 \right\})$ is minimized by the same value of $\beta$ as $f(\beta; \mathcal{T} = \left\{ \min (c) - 1 - x \right\})$.
	Similarly, $f(\beta; \mathcal{T} = \left\{ \max (d) \right\})$ has the same minimizing $\beta$ as $f(\beta; \mathcal{T} = \left\{ \max (d) + x \right\})$.
	However, from (the proof of) Proposition \ref{prop:variance_sum_epsilons}, we know that the variance of $\epsilon_{\max(d)} < \epsilon_{\max(d) + 1}$.
	Thus, the set $\mathcal{T} = \left\{ \max (d) + 1 \right\})$ contains the same (redundant) information as $\mathcal{T} = \left\{ \max (d) \right\})$ but comes with a higher variance.
	So it is always a better idea to use the set associated with the smaller variance.
	To summarize, each $x > 0$ provides unnecessary information that just inflates the variance of $f(\beta)$ compared to simply selecting $x=0$.
	$\square$
\end{proof}


\subsubsection{Analytical solutions for the unconstrained problem}


Just for some very simple special cases, we can find an analytical solution to the parameter estimation problem like in the following example.


\begin{example}
	For $N=1$, $J=1$, $T=3$, $L(x)=x^2$ and $\mathcal{T} = \left\{ 0, d \right\}$, the following closed-form solution for the parameter $\beta$ holds $\beta = \frac{ - D}{C \cdot R_{m,d}}$.
	
	In our simple case, the $\mathcal{T}$-averaged pricing error is
	\[
	\bar{\epsilon} = 0.5 \left( \Psi_{0,c} C + \Psi_{0,d} D \right) + 0.5 \left( \Psi_{d,c} C + \Psi_{d,d} D \right)
	\]
	Thus
	\[
	\bar{\epsilon}^2 = 0.25 \left( \Psi_{0,c} C + \Psi_{0,d} D + \Psi_{d,c} C + \Psi_{d,d} D \right)^2
	\]
	with
	\[
	\Psi_{u,v} = \frac{\prod_{t=0}^{u} \left( \beta \cdot R_{m,t} \right) }{\prod_{t=0}^{v} \left( \beta \cdot R_{m,t} \right)}
	\]
	equals
	\[
	\bar{\epsilon}^2 = 0.25 \left( 
	\frac{C}{\prod_{t=c}^{c} \left( \beta \cdot R_{m,t} \right)} + 
	\frac{D}{\prod_{t=c}^{d} \left( \beta \cdot R_{m,t} \right)} + 
	\prod_{t=d}^{d} \left( \beta \cdot R_{m,t} \right) C + 
	D 
	\right)^2
	\]
	The return products are rather simple in our two-period case
	\[
	\bar{\epsilon}^2 = 0.25 \left( 
	\frac{C}{ \left( \beta \cdot R_{m,c} \right)} + 
	\frac{D}{ \left( \beta \cdot R_{m,c} \right) \left( \beta \cdot R_{m,d} \right) } + 
	\left( \beta \cdot R_{m,d} \right) C + 
	D 
	\right)^2
	\]
	which we can rewrite as
	\[
	\bar{\epsilon}^2 = 0.25
	\frac{\left( \beta^2 R_{m,c} R_{m,d} +1 \right)^2 \left( \beta C R_{m,d} + D \right)^2}{\beta^4 R_{m,c}^2 R_{m,d}^2}
	\]
	Since the system is exactly identified, we can set $\bar{\epsilon}^2 = 0$.
	This is the case for $\beta = \frac{ - D}{C \cdot R_{m,d}}$ (and $\beta=\sqrt{ - \frac{1}{R_{m,c} R_{m,d}}}$ which yields a complex number).
	$\blacksquare$
\end{example}


For the overidentified case ($J>1$ with scalar parameter $\beta$, with $T>3$ and $c_1 \neq c_2$ or $d_1 \neq d_2$), generally no closed-form solution is possible anymore.
We generically show how to apply the pure Newton algorithm to approximate $\beta$.

\iffalse

\begin{definition}
	\label{def:newton_method}
	Newton method for scalar $\beta$ \cite[Section 5.1]{Beck14}: \\
	According to the (pure) Newton method, we iteratively approximate the optimal parameter $\beta^* \in \mathbb{R}$ by
	\[
	\beta_{n+1} = \beta_n - \frac{f'(\beta_n)}{f''(\beta_n)}
	\]
	where $n=0,1,2,\dots$ counts the algorithm iterations, and $\beta_0$ shall be some initial guess value.
	Here the first and second derivatives are defined as
	\[
	f'(\beta) = 
	\frac{\delta}{\delta \beta} f(\beta) 
	\]
	and
	\[
	f''(\beta) = 
	\frac{\delta^2}{\delta \beta^2} f(\beta) 
	\]
	If the parameter vector $\beta$ were multidimensional, $f'(\beta)$ has to be replaced by the gradient $\nabla f(\beta)$ and $\left[ f''(\beta) \right]^{-1}$ by the inverse of the Hessian matrix $\left[ \nabla^2 f(\beta) \right]^{-1}$.
\end{definition}


The next example shows that, due to the non-linearities associated with multi-period discounting, the first and second derivatives generally cannot be expressed by nice and compact formulas.
Even for our straightforward case, the derivative terms using the simple linear SDF become quite unhandy.


\begin{example}
	\label{ex:simple_linear_derivates_newton}
	For an over-identified system with $N=1$, $J=2$, $T=4$, and $\mathcal{T} = \left\{ 0 \right\}$, we can apply Newton's method as given by Definition \ref{def:newton_method} to approximate $\beta^*$. 
	
	We observe four cash flows $(c_1, C_1) (d_1, D_1), (c_2, C_2), (d_2. D_2)$ where the subscripts denote the fund identifier $j$.
	Now we assume a time lag between the fund cash flows $c_1=1$, $d_1=2$, $c_2=2$, and $d_2=3$.
	In other words, the first fund ($j=1$) buys an asset in period $t=1$ and sells it again at $t=2$.
	The second fund ($j=2$) buys another\footnote{Can it be also the same asset?} asset in period $t=2$ and sells it in $t=3$.
	So the funds are exposed to two different returns.
	In this case, with $\mathcal{T} = \left\{ 0 \right\}$ we have
	\[
	f (\beta) = 
	\sum_{j=1}^2 \left( \epsilon_j \right)^2 = 
	\sum_{j=1}^2
	\left( 
	\Psi_{0,c_j} C_j + 	\Psi_{0,d_j} D_j 
	\right)^2
	\]
	which yields
	\[
	\left( 
	\frac{C_1}{ \left( \beta \cdot R_{m,1} \right)} + 
	\frac{D_1}{ \left( \beta \cdot R_{m,1} \right) \left( \beta \cdot R_{m,2} \right) }
	\right)^2
	+
	\left( 
	\frac{C_2}{ \left( \beta \cdot R_{m,1} \right) \left( \beta \cdot R_{m,2} \right)} + 
	\frac{D_2}{ \left( \beta \cdot R_{m,1} \right) \left( \beta \cdot R_{m,2} \right) \left( \beta \cdot R_{m,3} \right) }
	\right)^2
	\]
	Here we obtain the following expression for the first derivative
	\[
	f'(\beta) = 
	\frac{\delta}{\delta \beta} f(\beta) =
	\frac{ -2 \cdot H }{\beta^7 R_{m,1}^2 R_{m,2}^2 R_{m,3}^2}
	\]
	with
	\[
	H = 
	\beta^4 C_1^2 R_{m,2}^2 R_{m,3}^2 + 3 \beta^3 C_1 D_1 R_{m,2} R_{m,3}^2 + 2 \beta^2 C_2^2 R_{m,3}^2
	+
	2 \beta^2 D_1^2 R_{m,3}^2 + 5 \beta C_2 D_2 R_{m,3} + 3 D_2^2
	\]
	The second derivative is naturally even longer
	\begin{multline}
		f''(\beta) = 
		\frac{\delta^2}{\delta \beta^2} f(\beta) =
		2 \left( - \frac{3 D_2}{\beta^4 R_{m,1} R_{m,2} R_{m,3} } - \frac{2 C_2}{\beta^3 R_{m,1} R_{m,2}} \right)^2 +  \\
		2 \left( - \frac{2 D_1}{\beta^3 R_{m,1} R_{m,2} } - \frac{C_1}{\beta^2 R_{m,1} } \right)^2 + 
		2 \left( \frac{6 D_1}{\beta^4 R_{m,1} R_{m,2} } + \frac{ 2 C_1}{\beta^3 R_{m,1} } \right)
		\left( \frac{D_1}{\beta^2 R_{m,1} R_{m,2} } + \frac{ C_1}{\beta R_{m,1} } \right) + \\
		2 \left( \frac{12 D_2}{\beta^5 R_{m,1} R_{m,2} R_{m,3} } + \frac{ 6 C_2}{\beta^4 R_{m,1} R_{m,2} } \right)
		\left( \frac{D_2}{\beta^3 R_{m,1} R_{m,2} R_{m,3} } + \frac{C_2}{\beta^2 R_{m,1} R_{m,2} } \right)
	\end{multline}
	Thus we can use the pure Newton method in this simple case. 
	$\blacksquare$
\end{example}


\subsubsection{Numerical solutions for the un/constrained problem}

In the previous paragraph, we learned that we face an unconstrained nonlinear optimization problem where (tractable expressions for) the gradient and Hessian are often not readily available.
Thus, it is often helpful to apply numerical optimization, e.g., so-called Quasi-Newton methods, to approximate the first and second derivative terms \cite[Chapter 6]{NW06}.
Similarly, the standard Gauss-Newton algorithm for nonlinear least-squares problems (i.e., $L(x)=x^2$) is not applicable since we cannot find easy analytical expressions for the Jacobian matrix \cite[Section 4.5]{Beck14}.
A (practically often) more robust but slower method than Quasi-Newton is the simplex algorithm of \cite{NM65}, which is also the default method of the \texttt{R} packages \texttt{optim} and \texttt{optimx} \cite[Sections 8.1 and 9.1]{N14}.

From our Theorem \ref{theo:consistency} point 2, we know that extremum estimators assume that the parameter space is compact.
This means the set of feasible parameter values for $\beta^*$ is bounded from below and above.
In turn, we (theoretically) face a constrained optimization problem
\[
\beta^* = \min_{\beta \in \left[ \beta_{\mathrm{min}}, \beta_{\mathrm{max}} \right]} f(\beta)
\]
where the constraint is admittedly rather simple.
Practically, it is more favorable to work with the unconstrained problem and a target function $f(\beta)$ that shows no exploding alpha or beta issues (cf. Proposition \ref{prop:exploding_beta} and Corollary \ref{coro:exploding_alpha}).
Methods to solve constrained optimization problems numerically are described by \cite{NW06} in chapters 12-15.

\textcolor{orange}{
	\begin{exercise}
		How can we show (mathematically rigorously) that it is too computationally costly to derive the analytical expression for the gradient and Hessian (even if the parameter is a scalar)?
		Alternatives to this symbolic/algebraic differentiation are automatic or numerical differentiation.
	\end{exercise}
}

\fi

\subsection{Exponential affine SDF}


In this subsection, let us assume an exponential affine SDF similar to \cite{KN16} instead of a simple linear one like by \cite{DLP12}.
With this new SDF assumption, we want to see how the results from Subsection \ref{subsec:newton_methods_nlo} change.

\begin{definition}
	\label{def:exp_aff_sdf}
	Exponential affine SDF: \\
	An exponential affine SDF is defined as
	\[
	\Psi_{u,v}^{\mathrm{EA}} =
	\frac{\prod_{t=0}^{u} \exp \left[ \beta \log \left( R_{m,t} \right) \right] }{\prod_{t=0}^{v} \exp \left[  \beta \log \left( R_{m,t} \right) \right]}
	\]
	which can be simplified to
	\[
	\Psi_{u,v}^{\mathrm{EA}} =
	\exp \left( \beta \left[ \sum_{t=0}^{u} \log \left( R_{m,t} \right) \right] \left[ -\sum_{t=0}^{v}  \log \left( R_{m,t} \right) \right] \right) = \exp \left( \beta \cdot Y_{u,v}^{(\mathrm{log})} \right)
	\]
	where we substitute $Y_{u,v}^{(\mathrm{log})} := \left[ \sum_{t=0}^{u} \log \left( R_{m,t} \right) \right] \left[ -\sum_{t=0}^{v}  \log \left( R_{m,t} \right) \right]$.
\end{definition}


We now analyze the exploding beta behavior for $\Psi_{u,v}^{\mathrm{EA}}$.

\begin{lemma}
	\label{lem:limit_exp_aff_sdf}
	With Definition \ref{def:exp_aff_sdf}, it holds
	\begin{enumerate}
		\item $	\lim_{\beta \to + \infty} \ \Psi_{u, v}^{\mathrm{EA}}  = 0 \quad \ \ \ $ when $ \quad Y_{u,v}^{(\mathrm{log})} < 0$,
		\item $	\lim_{\beta \to - \infty} \ \Psi_{u, v}^{\mathrm{EA}}  = 0 \quad \ \ \ $ when $ \quad Y_{u,v}^{(\mathrm{log})} > 0$,
		\item $	\lim_{\beta \to + \infty} \ \Psi_{u, v}^{\mathrm{EA}} = + \infty \quad$ when $ \quad Y_{u,v}^{(\mathrm{log})} > 0$,
		\item $	\lim_{\beta \to - \infty} \ \Psi_{u, v}^{\mathrm{EA}} = + \infty \quad$ when $ \quad Y_{u,v}^{(\mathrm{log})} < 0$.
	\end{enumerate}
	if $R_{m,t} \neq 1$ for at least one $t$. \\
	Further with an $\alpha$ term in the SDF $\Psi_{u,v}^{\mathrm{EA}, \alpha} = \exp \left( (u-v) \alpha + \beta \cdot Y_{u,v}^{(\mathrm{log})} \right)$, we have
	\begin{enumerate}
		\item $	\lim_{\alpha \to + \infty} \ \Psi_{u,v}^{\mathrm{EA}, \alpha}  = 0 \quad \ \ \ $ when $ \quad u < v$,
		\item $	\lim_{\alpha \to - \infty} \ \Psi_{u, v}^{\mathrm{EA}, \alpha}  = 0 \quad \ \ \ $ when $ \quad u > v$,
		\item $	\lim_{\alpha \to + \infty} \ \Psi_{u,v}^{\mathrm{EA}, \alpha}  = + \infty \quad $ when $ \quad u > v$,
		\item $	\lim_{\alpha \to - \infty} \ \Psi_{u, v}^{\mathrm{EA}, , \alpha}  = + \infty \quad $ when $ \quad u < v$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	The proof is straightforward since it is well known that $\lim_{x \to \infty} \exp(x) = \infty$ and $\lim_{x \to \infty} \exp(-x) = 0$. $\square$
\end{proof}


\textcolor{darkgreen}{
	Since $Y_{u,v}^{(\mathrm{log})}$ is solely determined by the market return $R_{m}$, the conditions that involve $Y_{u,v}^{(\mathrm{log})}$ in the first part of Lemma \ref{lem:limit_exp_aff_sdf} can be tested empirically.
	Further,} Lemma \ref{lem:limit_exp_aff_sdf} entails that for $\Psi_{u,v}^{\mathrm{EA}}$ (in contrast to $\Psi_{u,v}^{\mathrm{SL}}$) the exploding beta behavior depends on the return realizations $R_m$.
However, as we expect positive returns, especially over longer time horizons, we will likely observe (for basically all datasets) similar limiting properties for large $\beta$ as in the simple linear SDF case.
In other words, $\mathcal{T}$-averaging, as given by Definition \ref{def:tau_averaged_pricing_error}, with discounting dates larger than $\min(d)$ shall also be beneficial for the exponential affine SDF.

In the next example, we show that with Definition \ref{def:exp_aff_sdf} instead of Definition \ref{def:simple_linear_sdf} for the SDF we face a log-linear instead of a nonlinear problem.

\begin{example}
	We use almost the same setting as in Example \ref{ex:simple_linear_derivates_newton} which is $N=1$, $T=4$, $\mathcal{T} = \left\{ 0 \right\}$, and a quadratic loss function $L(x)=x^2$ to obtain for $f(\beta)$
	\[
	f(\beta) = 
	\sum_{j=1}^J \left[
	\exp( \beta Y_{0, c_{j}}^{(\mathrm{log})} ) C_{j} + 
	\exp( \beta Y_{0, d_{j}}^{(\mathrm{log})} ) D_{j}
	\right]^2
	\]
	The first derivative of $f(\beta)$ with respect to $\beta$ is then
	\[
	f'(\beta) = 
	\sum_{j=1}^J 2 H_{1,j} H_{2,j}
	\]
	where we substitute
	\[
	H_{1,j} = 	
	e^{\beta Y_{0, c_{j}}^{(\mathrm{log})}} C_{j} + 
	e^{\beta Y_{0, d_{j}}^{(\mathrm{log})}} D_{j}
	\]
	and
	\[
	H_{2,j} =
	e^{\beta Y_{0, c_{j}}^{(\mathrm{log})}} C_{j} Y_{0, c_{j}}^{(\mathrm{log})} + 
	e^{\beta Y_{0, d_{j}}^{(\mathrm{log})}} D_{j} Y_{0, d_{j}}^{(\mathrm{log})}
	\]
	The second derivative is
	\[
	f''(\beta) = \sum_{j=1}^J 2
	\left[
	\left( H_{2,j} \right)^2 + H_{1,j} H_{3,j}
	\right]
	\]
	where 
	\[
	H_{3,j} = 
	e^{\beta Y_{0, c_{j}}^{(\mathrm{log})}} C_{j} \left( Y_{0, c_{j}}^{(\mathrm{log})} \right)^2 + 
	e^{\beta Y_{0, d_{j}}^{(\mathrm{log})}} D_{j} \left( Y_{0, d_{j}}^{(\mathrm{log})} \right)^2
	\]
	In summary, we obtain more tractable expressions for the first and second derivatives when we use the exponential affine SDF.
	Thus, the pure Newton method can be much easier applied with $\Psi_{u, v}^{\mathrm{EA}}$ instead of $\Psi_{u, v}^{\mathrm{SL}}$.
	$\blacksquare$
\end{example}

However, in situations when computation time is not critical, it is not overly important if a pure Newton, Quasi-Newton or Simplex algorithm is applied to solve for the optimal $\beta^*$ value.
Usually it is still "faster" to use a general nonlinear minimizer (that requires no derivatives formulas) than to implement the theoretically most efficient estimation procedure.
Practically, it is always recommended to cross-check if multiple different estimation algorithms yield the same $\beta^*$ estimates.

To conclude, all results from this Section \ref{sec:sdf_estimation_cash_flows} can be viewed as theoretical foundation for Chapter \ref{chap:spatial_sdf_estimator}.

